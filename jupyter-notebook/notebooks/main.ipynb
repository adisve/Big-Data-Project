{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Comments from May 2015\n",
    "\n",
    "This dataset contains every publicly available Reddit comment for the month of May, 2015. Approximately 37 million comments were made by 2.7 million unique authors, stored in a ~15 GB compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/04 15:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------+\n",
      "|                body|score|subreddit|\n",
      "+--------------------+-----+---------+\n",
      "|Then you got your...| 6761|AskReddit|\n",
      "|Thanks man and al...| 5849|AskReddit|\n",
      "|We had an office ...| 5776|AskReddit|\n",
      "|OP are you alrigh...| 5767|AskReddit|\n",
      "|So she's pregnant...| 5762|AskReddit|\n",
      "|Forum based websi...| 5710|AskReddit|\n",
      "|It would be the i...| 5699|AskReddit|\n",
      "|         ¯\\\\_(ツ)_/¯| 5673|AskReddit|\n",
      "|HEY! THAT'S ME!!!...| 5642|    funny|\n",
      "|Being fucking lat...| 5570|AskReddit|\n",
      "+--------------------+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "hdfs_path = \"hdfs://namenode:9000/user/spark/reddit_comments/\"\n",
    "spark_master = \"spark://spark-master:7077\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(spark_master)\n",
    "    .appName(\"Reddit word analysis\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = (\n",
    "        spark.read.format(\"parquet\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(hdfs_path)\n",
    "    )\n",
    "\n",
    "df_copy = df\n",
    "\n",
    "df_copy = df[[\"body\", \"score\", \"subreddit\"]] # Drop any tables except these three\n",
    "df_copy.sort(\"score\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out non-latin characters, spaces, and null strings. Create new dataframe containing list of words in a given sentence (body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, lower, regexp_replace\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_copy = df_copy.filter(col(\"body\") != \"[deleted]\")\n",
    "df_copy = df_copy.filter(col(\"body\") != \"[removed]\")\n",
    "df_copy = df_copy.filter(col(\"body\").isNotNull() & (col(\"body\") != \"\"))\n",
    "\n",
    "latin_pattern = \"[^a-zA-Z\\s]\"\n",
    "\n",
    "words_df = df_copy.select(explode(split(lower(regexp_replace(col(\"body\"), latin_pattern, ' ')), \"\\s+\")).alias(\"word\"))\n",
    "words_df = words_df.filter(col(\"word\") != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch corpus of commonly used stopwords in english language and filter them out of any body of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_filtered_df = words_df.filter(~col(\"word\").isin(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the occurences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = words_filtered_df.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve top 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===========>                                              (8 + 1) / 41]\r"
     ]
    }
   ],
   "source": [
    "top_1000_words = word_counts.orderBy(\"count\", ascending=False).limit(1000).collect()\n",
    "word_freq = {word[\"word\"]: word[\"count\"] for word in top_1000_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cloud = WordCloud(background_color=\"white\", max_words=1000)\n",
    "cloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
